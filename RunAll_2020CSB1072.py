# -*- coding: utf-8 -*-
"""NewStartAss2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oIV9iwhwFbklmt3u2DkeQk4wHVW2xGY1
"""

#import libs
import matplotlib.pyplot as plt
from skimage import data
from skimage.color import rgb2gray
import cv2
import numpy as np
import tensorflow as tf
from tensorflow import keras
import random
import math
from statistics import mean
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score


target_names = ["T-shirt/top", "Trouser", "Pullover", "Dress",
                "Coat", "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

# load data
fashion_mnist = keras.datasets.fashion_mnist
(train_images, train_labels), (test_images,
                               test_labels) = fashion_mnist.load_data()
num_train_images = len(train_images)

# extract all descriptors and keypoints from training data to put them in a list
descriptors = []
keypoints = []
image_num = []
sift = cv2.xfeatures2d.SIFT_create()
len_arr = 0
for i in range(num_train_images):
    gray_img = rgb2gray(train_images[i])

    keypts, desc_img = sift.detectAndCompute(gray_img, None)
    len_arr = len(keypts)
    for j in range(len_arr):
        descriptors.append(desc_img[j])
        keypoints.append(keypts[j])
        image_num.append(i)

# make descriptors list a numpy array
descriptors = np.array(descriptors)


def CreateVisualDictionary(clusters, iter, descriptors, sensitivity=0.001):

    centroids = {}
    # assign initial cluster number of descriptors as intial centroids
    # for i in range(clusters):
    #   centroids[i] = descriptors[i]

    # select random centroids
    len_desc = len(descriptors)
    indices = random.sample(range(0, len_desc), clusters)
    for i in range(clusters):
        centroids[i] = descriptors[indices[i]]

    for iteration in range(iter):
        points_to_clusters = {}
        classifications = {}
        index_classifications = {}
        for i in range(clusters):
            classifications[i] = []
            index_classifications[i] = []
            points_to_clusters[i] = -1

        index = 0
        for desc in descriptors:
            distances = [np.linalg.norm(desc-centroids[i])
                         for i in range(clusters)]
            classification = distances.index(min(distances))
            classifications[classification].append(desc)
            index_classifications[classification].append(index)
            points_to_clusters[index] = classification
            index += 1

        old_centroids = (centroids)
        for classification in classifications:
            centroids[classification] = np.average(
                classifications[classification], axis=0)

        optimized = True
        for c in centroids:
            prev_centroid = old_centroids[c]
            new_centroid = centroids[c]
            if(np.sum((new_centroid-prev_centroid)/new_centroid*100) > sensitivity):
                optimized = False
        if(optimized):
            return centroids, classifications, index_classifications, points_to_clusters
            break


# cluster ind stores centroid
def find_nearest_cluster(centroids, feature):
    distances = [np.linalg.norm(feature-centroids[i]) for i in (centroids)]
    classification = distances.index(min(distances))
    return classification


def ComputeHistogram(centroids, features, len_features):
    frequency = [0 for i in centroids]
    for i in range(len_features):
        feature = features[i]
        classification = find_nearest_cluster(centroids, feature)
        frequency[classification] += 1
    frequency = np.array(frequency)
    return frequency

# uncomment to see plot of k vs wss
# def calculate_wss_score(points_to_clusters,centroids,descriptors):
#   pts= len(descriptors)
#   distance = [math.pow(np.linalg.norm(descriptors[i]-centroids[points_to_clusters[i]]),2) for i in (range(pts))]
#   wcc_score = np.sum(distance)
#   return wcc_score


# k_vs_wss = {i:0 for i in range(1,999)}

# test_k_for = 100

# for i in range(1,test_k_for,10):
#   if(k_vs_wss[i]==0):
#     centroids,classifications,index_classifications,points_to_clusters = CreateVisualDictionary(i,1,descriptors)
#     k_vs_wss[i] = calculate_wss_score(points_to_clusters,centroids,descriptors)
centroids, classifications, index_classifications, points_to_clusters = CreateVisualDictionary(
    50, 200, descriptors)

# for i in range(1,test_k_for,10):
#   print(i ," ",k_vs_wss[i])

# def plot_elbow(k_vs_wss,test_k_for):
#   fig = plt.figure(figsize = (10, 5))
#   new_dict = {}
#   for i in range(1,test_k_for,10):
#     new_dict[i] = k_vs_wss[i]
#   plt.plot(list(new_dict.keys()), list(new_dict.values()), color ='maroon')

#   plt.xlabel("K.")
#   plt.ylabel("wss")
#   plt.title("histogram")
#   plt.show()

# plot_elbow(k_vs_wss,test_k_for)


nearest_descriptor_index = {i: [] for i in centroids}


def find_nearest_descriptor(centroids, index_classifications):
    for centroid in centroids:
        distances = [np.linalg.norm(classifications[centroid][i]-centroids[centroid])
                     for i in (range(len(classifications[centroid])))]
        desc_index = distances.index(min(distances))
        nearest_descriptor_index[centroid] = index_classifications[centroid][desc_index]


find_nearest_descriptor(centroids, index_classifications)

print(nearest_descriptor_index)

# given feature set or histogram, we can plot it using this function


def plot_histogram(frequency, centroids):
    fig = plt.figure(figsize=(10, 5))

    plt.bar([i for i in centroids], [frequency[i]
                                     for i in centroids], color='maroon', width=0.4)

    plt.xlabel("cluster no.")
    plt.ylabel("frequency")
    plt.title("histogram")
    plt.show()

# Compute cossine similarity


def MatchHistogram(feature1, feature2):
    # 1: same distri
    # 0: different distri
    if(len(feature1) != len(feature2)):
        return 0
    dot_pro = np.dot(feature1, feature2)
    num = np.sum(dot_pro)
    deno = np.linalg.norm(feature1)*np.linalg.norm(feature2)
    if(deno == 0):
        return 0
    return num/deno


num_test_images = len(test_images)

predictions = np.full(num_test_images, -1)

feature_vectors = {list_val: [] for list_val in range(num_train_images)}

feature_vectors_test = {list_val: [] for list_val in range(num_test_images)}

test_for = num_test_images


def ComputeFeatureVectors(num_train_images, test_for):
    for i in range(num_train_images):
        if(feature_vectors[i] == []):
            gray_img = rgb2gray(train_images[i])
            keypts, desc_img = sift.detectAndCompute(gray_img, None)
            feature_vectors[i] = ComputeHistogram(
                centroids, desc_img, len(keypts))
    for i in range(test_for):
        if(feature_vectors_test[i] == []):
            gray_img = rgb2gray(test_images[i])
            keypts, desc_img = sift.detectAndCompute(gray_img, None)
            test_vec = ComputeHistogram(centroids, desc_img, len(keypts))
            feature_vectors_test[i] = test_vec


ComputeFeatureVectors(num_train_images, test_for)


def predict(test_for):
    for i in range(test_for):
        if(predictions[i] == -1):
            test_vec = feature_vectors_test[i]
            ind = -1
            cossine = [MatchHistogram(feature_vectors[j], test_vec)
                       for j in range(num_train_images)]
            ind = cossine.index(max(cossine))
            predictions[i] = train_labels[ind]


predict(test_for)


def calculate_overall_accuracy(predictions, test_labels, test_for):
    correct = 0
    total = test_for
    for i in range(test_for):
        if(predictions[i] == test_labels[i]):
            correct += 1
    return (correct/total)*100


y_true = test_labels[:test_for]
y_predict = predictions[:test_for]
print(classification_report(y_true, y_predict, target_names=target_names))
print("Overall accuracy => ")
print(accuracy_score(y_true, y_predict))
